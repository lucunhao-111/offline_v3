import requests
from bs4 import BeautifulSoup
import time
import hashlib
import json
import re
import os
from datetime import datetime

# åŸå¸‚ä»£ç æ˜ å°„ï¼ˆå¯ä»¥ä»é¡µé¢è·å–æˆ–æ‰‹åŠ¨å®šä¹‰ï¼‰
CITY_CODES = {
    "åŒ—äº¬": "54511",
    "ä¸Šæµ·": "58367",
    "å¹¿å·": "59287",
    "æ·±åœ³": "59493",
    "æ­å·": "58457",
    "å—äº¬": "58238",
    "æˆéƒ½": "56294",
    "æ­¦æ±‰": "57494",
    "è¥¿å®‰": "57036",
    "é‡åº†": "57516"
}

# æˆ–è€…ä»é¡µé¢åŠ¨æ€è·å–åŸå¸‚åˆ—è¡¨
BASE_URL = "https://weather.cma.cn/web/weather/{}.html"
DATA_DIR = "weather_data"
history_weather_data = {}

def ensure_data_directory():
    """ç¡®ä¿æ•°æ®ç›®å½•å­˜åœ¨"""
    if not os.path.exists(DATA_DIR):
        os.makedirs(DATA_DIR)
        print(f"ğŸ“ åˆ›å»ºæ•°æ®ç›®å½•: {DATA_DIR}")
    return DATA_DIR

def get_city_list():
    """ä»é¡µé¢è·å–åŸå¸‚åˆ—è¡¨"""
    print("ğŸ”„ æ­£åœ¨è·å–åŸå¸‚åˆ—è¡¨...")
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
    }

    try:
        # è®¿é—®ä¸»é¡µè·å–åŸå¸‚åˆ—è¡¨
        main_url = "https://weather.cma.cn/web/weather/58367.html"  # ä¸Šæµ·ä½œä¸ºå…¥å£
        response = requests.get(main_url, headers=headers, timeout=15)
        response.encoding = 'utf-8'
        soup = BeautifulSoup(response.text, "html.parser")

        cities = {}

        # æ–¹æ³•1: ä»ä¸‹æ‹‰èœå•è·å–åŸå¸‚
        select_elements = soup.find_all("select")
        for select in select_elements:
            options = select.find_all("option")
            for option in options:
                city_name = option.get_text(strip=True)
                city_value = option.get('value', '')
                if city_name and city_name not in ['è¯·é€‰æ‹©', 'åŸå¸‚'] and len(city_name) <= 4:
                    # æå–åŸå¸‚ä»£ç 
                    code_match = re.search(r'/(\d+)\.html', city_value)
                    if code_match:
                        cities[city_name] = code_match.group(1)

        # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œä½¿ç”¨é¢„å®šä¹‰çš„åŸå¸‚åˆ—è¡¨
        if not cities:
            print("âš ï¸ æ— æ³•ä»é¡µé¢è·å–åŸå¸‚åˆ—è¡¨ï¼Œä½¿ç”¨é¢„å®šä¹‰åˆ—è¡¨")
            return CITY_CODES

        print(f"âœ… æ‰¾åˆ° {len(cities)} ä¸ªåŸå¸‚")
        return cities

    except Exception as e:
        print(f"âŒ è·å–åŸå¸‚åˆ—è¡¨å¤±è´¥: {e}")
        print("âš ï¸ ä½¿ç”¨é¢„å®šä¹‰åŸå¸‚åˆ—è¡¨")
        return CITY_CODES

def get_weather_data(city_code, city_name):
    """è·å–æŒ‡å®šåŸå¸‚çš„å¤©æ°”æ•°æ®"""

    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
    }

    try:
        city_url = BASE_URL.format(city_code)
        response = requests.get(city_url, headers=headers, timeout=15)
        response.encoding = 'utf-8'
        response.raise_for_status()

        soup = BeautifulSoup(response.text, "html.parser")

        # è§£æ7å¤©å¤©æ°”é¢„æŠ¥
        forecast_list = parse_7day_forecast(soup)

        # è§£æå®æ—¶å¤©æ°”ä¿¡æ¯
        realtime_data = parse_realtime_weather(soup)

        current_data = {
            "location": {
                "city": city_name,
                "city_code": city_code
            },
            "realtime": realtime_data,
            "forecast_7d": forecast_list,
            "update_time": time.strftime("%Y-%m-%d %H:%M:%S", time.localtime()),
            "source": city_url
        }

        print(f"âœ… è·å– {city_name} æ•°æ®æˆåŠŸ - æ¸©åº¦: {realtime_data.get('temp', 'N/A')}")
        return current_data

    except Exception as e:
        print(f"âŒ è·å– {city_name} æ°”è±¡æ•°æ®å¤±è´¥ï¼š{str(e)}")
        return None

def parse_7day_forecast(soup):
    """è§£æ7å¤©å¤©æ°”é¢„æŠ¥"""
    forecast_list = []

    days_container = soup.find("div", class_="row hb days")
    if not days_container:
        return []

    day_divs = days_container.find_all("div", class_=re.compile(r"pull-left day"))

    for day_div in day_divs:
        try:
            date_text = day_div.get_text(strip=True)

            date_match = re.search(r'([å‘¨ä¸€äºŒä¸‰å››äº”å…­æ—¥]+)(\d{2}/\d{2})', date_text)
            date_str = date_match.group(0) if date_match else "æœªçŸ¥æ—¥æœŸ"

            weather_match = re.search(r'([å¤šäº‘æ™´é˜´é›¨é›ªæš´é›¾æ²™å°˜é›·]+)', date_text)
            weather_str = weather_match.group(1) if weather_match else "æœªçŸ¥"

            wind_match = re.search(r'([ä¸œå—è¥¿åŒ—]+é£)', date_text)
            wind_str = wind_match.group(1) if wind_match else "æœªçŸ¥"

            wind_level_match = re.search(r'(å¾®é£|è½»é£|å¼ºé£|\d+-\d+çº§é£?)', date_text)
            wind_level = wind_level_match.group(1) if wind_level_match else "æœªçŸ¥"

            temp_matches = re.findall(r'(\d+)â„ƒ', date_text)
            if len(temp_matches) >= 2:
                high_temp = f"{temp_matches[0]}â„ƒ"
                low_temp = f"{temp_matches[1]}â„ƒ"
                temp_range = f"{low_temp}~{high_temp}"
            else:
                temp_range = "æœªçŸ¥"
                high_temp = "æœªçŸ¥"
                low_temp = "æœªçŸ¥"

            forecast_list.append({
                "date": date_str,
                "weather": weather_str,
                "wind_direction": wind_str,
                "wind_level": wind_level,
                "temp_range": temp_range,
                "high_temp": high_temp,
                "low_temp": low_temp
            })

        except Exception as e:
            continue

    return forecast_list

def parse_realtime_weather(soup):
    """è§£æå®æ—¶å¤©æ°”ä¿¡æ¯"""
    realtime_data = {}

    active_day = soup.find("div", class_="pull-left day actived")
    if active_day:
        active_text = active_day.get_text(strip=True)

        temp_matches = re.findall(r'(\d+)â„ƒ', active_text)
        if temp_matches:
            realtime_data['temp'] = f"{temp_matches[0]}â„ƒ"

        weather_match = re.search(r'([å¤šäº‘æ™´é˜´é›¨é›ªæš´é›¾æ²™å°˜é›·]+)', active_text)
        if weather_match:
            realtime_data['weather'] = weather_match.group(1)

        wind_match = re.search(r'([ä¸œå—è¥¿åŒ—]+é£)', active_text)
        if wind_match:
            realtime_data['wind_direction'] = wind_match.group(1)

        wind_level_match = re.search(r'(å¾®é£|è½»é£|å¼ºé£|\d+-\d+çº§é£?)', active_text)
        if wind_level_match:
            realtime_data['wind_level'] = wind_level_match.group(1)
        else:
            realtime_data['wind_level'] = "æœªçŸ¥"

    page_text = soup.get_text()

    humidity_match = re.search(r'æ¹¿åº¦\s*(\d+)%?', page_text)
    realtime_data['humidity'] = f"{humidity_match.group(1)}%" if humidity_match else "æœªçŸ¥"

    pressure_match = re.search(r'æ°”å‹\s*(\d+)\s*(h?Pa?)', page_text, re.IGNORECASE)
    realtime_data['pressure'] = f"{pressure_match.group(1)}{pressure_match.group(2)}" if pressure_match else "æœªçŸ¥"

    if 'temp' not in realtime_data:
        temp_matches = re.findall(r'(\d+\s*â„ƒ)', page_text)
        realtime_data['temp'] = temp_matches[0] if temp_matches else "æœªçŸ¥"

    return realtime_data

def is_weather_data_changed(new_data, old_data, city_name):
    """åˆ¤æ–­å¤©æ°”æ•°æ®æ˜¯å¦çœŸæ­£å‘ç”Ÿå˜åŒ–"""
    if city_name not in old_data:
        return True
    if new_data is None:
        return False

    try:
        new_copy = new_data.copy()
        old_copy = old_data[city_name].copy()

        new_copy.pop('update_time', None)
        old_copy.pop('update_time', None)

        new_hash = hashlib.md5(json.dumps(new_copy, sort_keys=True).encode()).hexdigest()
        old_hash = hashlib.md5(json.dumps(old_copy, sort_keys=True).encode()).hexdigest()

        return new_hash != old_hash

    except Exception as e:
        print(f"æ¯”è¾ƒæ•°æ®æ—¶å‡ºé”™: {e}")
        return True

def save_weather_data(data):
    """ä¿å­˜æ•°æ®åˆ°æŒ‡å®šç›®å½•"""
    if data:
        try:
            data_dir = ensure_data_directory()

            city_name = data['location'].get('city', 'æœªçŸ¥åœ°åŒº')
            safe_city_name = re.sub(r'[\\/*?:"<>|]', '', city_name)

            current_time = datetime.now()
            date_str = current_time.strftime("%Y%m%d")
            time_str = current_time.strftime("%H%M%S")

            filename = f"weather_{safe_city_name}_{date_str}_{time_str}.json"
            filepath = os.path.join(data_dir, filename)

            with open(filepath, "w", encoding="utf-8") as f:
                json.dump(data, f, ensure_ascii=False, indent=2)

            print(f"ğŸ’¾ {city_name} æ•°æ®å·²ä¿å­˜: {filename}")
            print(f"ğŸ“Š å®æ—¶æ¸©åº¦: {data['realtime'].get('temp', 'æœªçŸ¥')}, å¤©æ°”: {data['realtime'].get('weather', 'æœªçŸ¥')}")

            return filepath

        except Exception as e:
            print(f"âŒ ä¿å­˜å¤±è´¥: {str(e)}")
            return None

def cleanup_old_files(max_files_per_city=20):
    """æ¸…ç†æ—§æ–‡ä»¶ï¼Œæ¯ä¸ªåŸå¸‚ä¿ç•™æœ€æ–°çš„æ–‡ä»¶"""
    try:
        data_dir = ensure_data_directory()
        files = [f for f in os.listdir(data_dir) if f.endswith('.json')]

        # æŒ‰åŸå¸‚åˆ†ç»„
        city_files = {}
        for f in files:
            city_match = re.search(r'weather_([^_]+)_', f)
            if city_match:
                city = city_match.group(1)
                if city not in city_files:
                    city_files[city] = []
                filepath = os.path.join(data_dir, f)
                mtime = os.path.getmtime(filepath)
                city_files[city].append((mtime, filepath))

        # å¯¹æ¯ä¸ªåŸå¸‚æ¸…ç†æ—§æ–‡ä»¶
        for city, file_list in city_files.items():
            if len(file_list) > max_files_per_city:
                file_list.sort(reverse=True)  # æŒ‰æ—¶é—´å€’åºï¼Œæœ€æ–°çš„åœ¨å‰é¢
                files_to_delete = file_list[max_files_per_city:]
                for mtime, filepath in files_to_delete:
                    os.remove(filepath)
                    print(f"ğŸ—‘ï¸  æ¸…ç† {city} çš„æ—§æ–‡ä»¶: {os.path.basename(filepath)}")

    except Exception as e:
        print(f"æ¸…ç†æ–‡ä»¶æ—¶å‡ºé”™: {e}")

def main():
    """ä¸»å‡½æ•° - åŸå¸‚éå†ç‰ˆæœ¬"""
    print("ğŸŒ¤ï¸  å¼€å§‹å¤šåŸå¸‚æ°”è±¡æ•°æ®ç›‘æ§...")
    data_dir = ensure_data_directory()
    print(f"æ•°æ®ä¿å­˜ç›®å½•: {os.path.abspath(data_dir)}")

    # è·å–åŸå¸‚åˆ—è¡¨
    cities = get_city_list()
    print(f"ğŸ¯ ç›‘æ§åŸå¸‚åˆ—è¡¨: {', '.join(cities.keys())}")

    print("\nğŸ”„ å¼€å§‹åŸå¸‚éå†å¾ªç¯...")
    cycle_count = 0
    forward = True  # æ§åˆ¶éå†æ–¹å‘

    while True:
        try:
            cycle_count += 1
            print(f"\n{'='*50}")
            print(f"ğŸš€ ç¬¬{cycle_count}è½®éå† ({'æ­£åº' if forward else 'å€’åº'})")
            print(f"â° å¼€å§‹æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}")
            print(f"{'='*50}")

            # æ ¹æ®éå†æ–¹å‘ç¡®å®šåŸå¸‚é¡ºåº
            city_items = list(cities.items())
            if not forward:
                city_items.reverse()

            for city_name, city_code in city_items:
                print(f"\nğŸ“ æ­£åœ¨è·å– {city_name} çš„å¤©æ°”æ•°æ®...")

                current_weather = get_weather_data(city_code, city_name)

                if current_weather:
                    if is_weather_data_changed(current_weather, history_weather_data, city_name):
                        filepath = save_weather_data(current_weather)
                        if filepath:
                            history_weather_data[city_name] = current_weather
                            print(f"ğŸ”„ {city_name} æ•°æ®å·²æ›´æ–°")
                        else:
                            print(f"âš ï¸  {city_name} æ•°æ®æœ‰å˜åŒ–ä½†ä¿å­˜å¤±è´¥")
                    else:
                        print(f"â¸ï¸  {city_name} æ•°æ®æ— å˜åŒ–")
                else:
                    print(f"âŒ è·å– {city_name} æ•°æ®å¤±è´¥")

                # åŸå¸‚é—´é—´éš”2ç§’ï¼Œé¿å…è¯·æ±‚è¿‡å¿«
                time.sleep(2)

            # åˆ‡æ¢éå†æ–¹å‘
            forward = not forward

            print(f"\nâœ… ç¬¬{cycle_count}è½®éå†å®Œæˆ")
            print(f"â° å®Œæˆæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}")
            print(f"â³ ç­‰å¾…10ç§’åå¼€å§‹ä¸‹ä¸€è½®...")

            # æ¯5è½®æ¸…ç†ä¸€æ¬¡æ–‡ä»¶
            if cycle_count % 5 == 0:
                cleanup_old_files(max_files_per_city=10)

            time.sleep(10)

        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ ç”¨æˆ·ä¸­æ–­ç¨‹åº")
            break
        except Exception as e:
            print(f"âŒ ç›‘æ§å¾ªç¯å‡ºé”™: {e}")
            time.sleep(10)

if __name__ == "__main__":
    main()